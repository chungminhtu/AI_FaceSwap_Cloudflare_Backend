#!/usr/bin/env node
/**
 * Script to extract a zip file and upload all contents to Cloudflare R2
 * Maintains exact folder structure from the zip file
 * Overwrites existing files in R2
 * Uses parallel uploads for maximum speed (default: 100 concurrent uploads)
 * 
 * Usage:
 *   node upload-zip-to-r2.js <zip-file-path> [environment] [bucket-name] [prefix] [concurrency]
 * 
 * Examples:
 *   node upload-zip-to-r2.js ./files.zip ai-office
 *   node upload-zip-to-r2.js ./files.zip ai-office-dev faceswap-images-office-dev
 *   node upload-zip-to-r2.js ./files.zip ai-office faceswap-images preset/
 *   node upload-zip-to-r2.js ./files.zip ai-office faceswap-images "" 150
 * 
 * Concurrency: Number of parallel uploads (default: 100, max: 200)
 */

const fs = require('fs');
const path = require('path');
const { spawn, execSync } = require('child_process');
const JSZip = require('jszip');
const os = require('os');

// Parse command line arguments
const zipFilePath = process.argv[2];
const environment = process.argv[3] || 'ai-office';
const bucketName = process.argv[4] || null; // Will be read from config if not provided
// Handle prefix and concurrency - if 5th arg is a number, it's concurrency (no prefix)
// Otherwise, 5th is prefix and 6th is concurrency
let prefix = '';
let concurrencyArg;
if (process.argv[5] && !isNaN(parseInt(process.argv[5], 10))) {
  // 5th arg is a number, so it's concurrency (no prefix)
  concurrencyArg = process.argv[5];
} else {
  // 5th arg is prefix (or empty), 6th is concurrency
  prefix = process.argv[5] || '';
  concurrencyArg = process.argv[6];
}
const concurrency = concurrencyArg ? parseInt(concurrencyArg, 10) : 100; // Default to 100 parallel uploads

if (!zipFilePath) {
  console.error('Usage: node upload-zip-to-r2.js <zip-file-path> [environment] [bucket-name] [prefix] [concurrency]');
  console.error('  concurrency: Number of parallel uploads (default: 100, max: 200)');
  process.exit(1);
}

if (isNaN(concurrency) || concurrency < 1 || concurrency > 200) {
  console.error('Error: Concurrency must be a number between 1 and 200');
  process.exit(1);
}

// Check if zip file exists
if (!fs.existsSync(zipFilePath)) {
  console.error(`Error: Zip file not found: ${zipFilePath}`);
  process.exit(1);
}

// Load deployment secrets
const secretsPath = path.join(__dirname, 'deployments-secrets.json');
if (!fs.existsSync(secretsPath)) {
  console.error(`Error: Deployment secrets not found: ${secretsPath}`);
  process.exit(1);
}

const secrets = JSON.parse(fs.readFileSync(secretsPath, 'utf8'));
const envConfig = secrets.environments?.[environment];

if (!envConfig?.cloudflare) {
  console.error(`Error: No Cloudflare config found for environment: ${environment}`);
  process.exit(1);
}

const { apiToken, accountId } = envConfig.cloudflare;
if (!apiToken || !accountId) {
  console.error(`Error: Missing API token or account ID for environment: ${environment}`);
  process.exit(1);
}

// Use bucket name from config if not provided
const targetBucket = bucketName || envConfig.bucketName;
if (!targetBucket) {
  console.error(`Error: Bucket name not specified and not found in config for environment: ${environment}`);
  process.exit(1);
}

// Set environment variables for wrangler
process.env.CLOUDFLARE_API_TOKEN = apiToken;
process.env.CLOUDFLARE_ACCOUNT_ID = accountId;

// Create temporary directory for extraction
const tempDir = fs.mkdtempSync(path.join(os.tmpdir(), 'zip-upload-'));

console.log(`\nüì¶ Extracting zip file: ${zipFilePath}`);
console.log(`üìÅ Temporary directory: ${tempDir}`);
console.log(`‚òÅÔ∏è  Target R2 bucket: ${targetBucket}`);
console.log(`üåç Environment: ${environment}`);
if (prefix) {
  console.log(`üìÇ Prefix: ${prefix}`);
}
console.log(`‚ö° Concurrency: ${concurrency} parallel uploads`);
console.log('');

async function extractZip(zipPath, extractTo) {
  try {
    const zipData = fs.readFileSync(zipPath);
    const zip = await JSZip.loadAsync(zipData);
    
    const files = [];
    const extractPromises = [];
    
    // Extract all files
    for (const [relativePath, zipEntry] of Object.entries(zip.files)) {
      if (zipEntry.dir) {
        // Create directory
        const dirPath = path.join(extractTo, relativePath);
        if (!fs.existsSync(dirPath)) {
          fs.mkdirSync(dirPath, { recursive: true });
        }
      } else {
        // Extract file
        const filePath = path.join(extractTo, relativePath);
        const dirPath = path.dirname(filePath);
        
        if (!fs.existsSync(dirPath)) {
          fs.mkdirSync(dirPath, { recursive: true });
        }
        
        extractPromises.push(
          zipEntry.async('nodebuffer').then(buffer => {
            fs.writeFileSync(filePath, buffer);
            files.push({
              localPath: filePath,
              r2Key: relativePath.replace(/\\/g, '/'), // Normalize path separators
              size: buffer.length
            });
          })
        );
      }
    }
    
    await Promise.all(extractPromises);
    return files;
  } catch (error) {
    throw new Error(`Failed to extract zip: ${error.message}`);
  }
}

async function uploadFileToR2WithWrangler(localPath, r2Key, bucket, prefix, maxRetries = 5) {
  return new Promise((resolve, reject) => {
    // Construct the full R2 key with prefix
    const fullKey = prefix ? `${prefix.replace(/\/$/, '')}/${r2Key}` : r2Key;
    
    // Wrangler command: wrangler r2 object put "bucket/key" --file "local-path"
    const objectPath = `${bucket}/${fullKey}`;
    const args = ['r2', 'object', 'put', objectPath, '--file', localPath];
    
    const env = {
      ...process.env,
      CLOUDFLARE_API_TOKEN: apiToken,
      CLOUDFLARE_ACCOUNT_ID: accountId
    };
    
    let attempt = 0;
    
    const tryUpload = () => {
      attempt++;
      const wrangler = spawn('wrangler', args, {
        env,
        stdio: ['ignore', 'pipe', 'pipe']
      });
      
      let stdout = '';
      let stderr = '';
      
      wrangler.stdout.on('data', (data) => {
        stdout += data.toString();
      });
      
      wrangler.stderr.on('data', (data) => {
        stderr += data.toString();
      });
      
      wrangler.on('close', (code) => {
        if (code === 0) {
          resolve(true);
        } else {
          const errorMsg = stderr || stdout || 'Unknown error';
          
          // Check if it's a retryable error
          const isRetryable = 
            errorMsg.includes('rate limit') || 
            errorMsg.includes('Rate limited') || 
            errorMsg.includes('429') ||
            errorMsg.includes('500') ||
            errorMsg.includes('response.statusCode = 500') ||
            errorMsg.includes('Network connection lost') ||
            errorMsg.includes('connection lost') ||
            errorMsg.includes('timeout') ||
            errorMsg.includes('ECONNRESET') ||
            errorMsg.includes('ETIMEDOUT') ||
            errorMsg.includes('Unspecified error');
          
          if (isRetryable && attempt < maxRetries) {
            // Exponential backoff: wait 1s, 2s, 4s, 8s
            const waitTime = Math.pow(2, attempt - 1) * 1000;
            setTimeout(() => {
              tryUpload();
            }, waitTime);
            return;
          }
          
          reject(new Error(errorMsg.substring(0, 200)));
        }
      });
      
      wrangler.on('error', (error) => {
        // Network errors are retryable
        if (attempt < maxRetries) {
          const waitTime = Math.pow(2, attempt - 1) * 1000;
          setTimeout(() => {
            tryUpload();
          }, waitTime);
        } else {
          reject(error);
        }
      });
    };
    
    tryUpload();
  });
}

function checkRcloneAvailable() {
  try {
    execSync('which rclone', { stdio: 'ignore' });
    return true;
  } catch {
    return false;
  }
}

async function uploadFolderWithRclone(tempDir, bucket, prefix, accountId) {
  return new Promise((resolve, reject) => {
    const endpoint = `https://${accountId}.r2.cloudflarestorage.com`;
    
    console.log('üöÄ Using rclone for fast folder upload...\n');
    
    const sourcePath = tempDir;
    // Use bucket name as remote, or check for R2_REMOTE env var
    const remoteName = process.env.R2_REMOTE || bucket;
    const destPath = prefix 
      ? `${remoteName}:${prefix.replace(/\/$/, '')}/` 
      : `${remoteName}:`;
    
    // Use rclone copy with high concurrency for fast uploads
    const args = [
      'copy',
      sourcePath,
      destPath,
      '--transfers', '50',  // 50 parallel file transfers
      '--checkers', '50',   // 50 parallel checks
      '--progress',
      '--stats', '1s'       // Show stats every second
    ];
    
    // If using inline S3 config (requires access keys)
    if (process.env.R2_ACCESS_KEY_ID && process.env.R2_SECRET_ACCESS_KEY) {
      args.push('--s3-endpoint', endpoint);
      args.push('--s3-provider', 'Cloudflare');
      args.push('--s3-access-key-id', process.env.R2_ACCESS_KEY_ID);
      args.push('--s3-secret-access-key', process.env.R2_SECRET_ACCESS_KEY);
      args.push('--s3-region', 'auto');
    }
    
    const rclone = spawn('rclone', args, {
      env: process.env,
      stdio: 'inherit'
    });
    
    rclone.on('close', (code) => {
      if (code === 0) {
        resolve(true);
      } else {
        reject(new Error(`rclone upload failed with code ${code}. Make sure rclone is configured with: rclone config`));
      }
    });
    
    rclone.on('error', (error) => {
      reject(error);
    });
  });
}

async function uploadBatch(files, bucket, prefix, concurrency = 20) {
  let successCount = 0;
  let failCount = 0;
  const failedFiles = [];
  let completed = 0;
  
  // Process files with concurrency limit using a queue
  const queue = [...files];
  const active = new Set();
  
  const processNext = async () => {
    if (queue.length === 0 && active.size === 0) {
      return;
    }
    
    if (queue.length === 0 || active.size >= concurrency) {
      return;
    }
    
    const file = queue.shift();
    active.add(file);
    
    const uploadPromise = (async () => {
      try {
        await uploadFileToR2WithWrangler(file.localPath, file.r2Key, bucket, prefix);
        completed++;
        const progress = `[${completed}/${files.length}]`;
        console.log(`${progress} ‚úÖ ${file.r2Key} (${(file.size / 1024).toFixed(2)} KB)`);
        successCount++;
      } catch (error) {
        completed++;
        const progress = `[${completed}/${files.length}]`;
        console.error(`${progress} ‚ùå ${file.r2Key}: ${error.message}`);
        failCount++;
        failedFiles.push({ key: file.r2Key, error: error.message });
      } finally {
        active.delete(file);
        // Process next file
        await processNext();
      }
    })();
    
    // Process next file immediately if we have capacity
    if (active.size < concurrency) {
      processNext();
    }
  };
  
  // Start initial batch
  const initialBatch = Math.min(concurrency, files.length);
  for (let i = 0; i < initialBatch; i++) {
    processNext();
  }
  
  // Wait for all uploads to complete
  while (active.size > 0 || queue.length > 0) {
    await new Promise(resolve => setTimeout(resolve, 100));
  }
  
  return { successCount, failCount, failedFiles };
}

async function main() {
  try {
    // Extract zip file
    const files = await extractZip(zipFilePath, tempDir);
    
    console.log(`‚úÖ Extracted ${files.length} files\n`);
    
    const startTime = Date.now();
    let successCount, failCount, failedFiles;
    
    // Try to use rclone for folder upload (much faster)
    // rclone can be used if:
    // 1. rclone is installed AND
    // 2. Either R2_ACCESS_KEY_ID/R2_SECRET_ACCESS_KEY are set OR rclone is pre-configured
    const rcloneAvailable = checkRcloneAvailable();
    const hasAccessKeys = process.env.R2_ACCESS_KEY_ID && process.env.R2_SECRET_ACCESS_KEY;
    const useRclone = rcloneAvailable && hasAccessKeys;
    
    if (useRclone) {
      try {
        console.log('üì§ Uploading folder to R2 using rclone (fast batch upload)...\n');
        await uploadFolderWithRclone(tempDir, targetBucket, prefix, accountId);
        successCount = files.length;
        failCount = 0;
        failedFiles = [];
        console.log(`\n‚úÖ Successfully uploaded ${files.length} files using rclone\n`);
      } catch (rcloneError) {
        console.log(`\n‚ö†Ô∏è  rclone upload failed: ${rcloneError.message}`);
        console.log('üì§ Falling back to wrangler (file-by-file upload)...\n');
        console.log(`‚ö° Using ${concurrency} concurrent uploads\n`);
        
        const result = await uploadBatch(
          files, 
          targetBucket, 
          prefix,
          concurrency
        );
        successCount = result.successCount;
        failCount = result.failCount;
        failedFiles = result.failedFiles;
      }
    } else {
      // Use wrangler (file-by-file)
      console.log('üì§ Uploading files to R2 in parallel batches (using wrangler)...\n');
      console.log(`‚ö° Using ${concurrency} concurrent uploads for maximum speed\n`);
      if (rcloneAvailable && !hasAccessKeys) {
        console.log('üí° Tip: Set R2_ACCESS_KEY_ID and R2_SECRET_ACCESS_KEY env vars to use rclone for faster folder uploads\n');
      } else if (!rcloneAvailable) {
        console.log('üí° Tip: Install rclone (brew install rclone) and set R2_ACCESS_KEY_ID/R2_SECRET_ACCESS_KEY for faster folder uploads\n');
      }
      
      const result = await uploadBatch(
        files, 
        targetBucket, 
        prefix,
        concurrency
      );
      successCount = result.successCount;
      failCount = result.failCount;
      failedFiles = result.failedFiles;
    }
    const endTime = Date.now();
    const duration = ((endTime - startTime) / 1000).toFixed(2);
    
    // Cleanup temporary directory
    console.log('\nüßπ Cleaning up temporary files...');
    fs.rmSync(tempDir, { recursive: true, force: true });
    
    // Summary
    console.log('\n' + '='.repeat(60));
    console.log('üìä Upload Summary:');
    console.log(`   ‚úÖ Success: ${successCount} files`);
    console.log(`   ‚ùå Failed: ${failCount} files`);
    console.log(`   üì¶ Total: ${files.length} files`);
    console.log(`   ‚è±Ô∏è  Duration: ${duration} seconds`);
    console.log(`   üöÄ Speed: ${(files.length / parseFloat(duration)).toFixed(2)} files/second`);
    console.log('='.repeat(60));
    
    if (failedFiles.length > 0) {
      console.log('\n‚ùå Failed files (first 10):');
      failedFiles.slice(0, 10).forEach(({ key, error }) => {
        console.log(`   - ${key}: ${error}`);
      });
      if (failedFiles.length > 10) {
        console.log(`   ... and ${failedFiles.length - 10} more failed files`);
      }
      process.exit(1);
    } else {
      console.log('\nüéâ All files uploaded successfully!');
    }
    
  } catch (error) {
    console.error(`\n‚ùå Error: ${error.message}`);
    console.error(error.stack);
    
    // Cleanup on error
    if (fs.existsSync(tempDir)) {
      fs.rmSync(tempDir, { recursive: true, force: true });
    }
    
    process.exit(1);
  }
}

// Run the script
main();


